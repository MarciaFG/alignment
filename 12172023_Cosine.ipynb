{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNVil8I1fVlvl5KP6wTdiSU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7VMi1PuCNN8",
        "outputId": "9c97f19c-fb1c-4b61-be8b-3cea78f8c8fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 14 20:57:45 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   29C    P0              45W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "metadata": {
        "id": "upwqQoBID0TC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f89aa1-6996-4773-b8fc-59d271855f8c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Authenticated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "T9413G4JXbEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Loading the datasets\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "\n",
        "column_names = [\n",
        "    'researcher_id',\n",
        "    'grid_id',\n",
        "    'pub_year',\n",
        "    'pubs',\n",
        "    'cluster_id1',\n",
        "    'LR_main_field_id',\n",
        "    'LR_main_field',\n",
        "    'native',\n",
        "    'outgoer',\n",
        "    'newcomer'\n",
        "]\n",
        "\n",
        "PATH = '/content/drive/My Drive/revisions_natcomms/data/raw_vector_df.zip'\n",
        "\n",
        "with zipfile.ZipFile(PATH, 'r') as z:\n",
        "    with z.open('raw_vector_df.csv') as f:\n",
        "        df = pd.read_csv(f, sep = ';', header=None, names=column_names)\n",
        "\n",
        "print(df.head(1))"
      ],
      "metadata": {
        "id": "wcLb-_MWD16x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "75221d1d-493a-4610-a4d7-34c5299d0367"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-77fb1e424138>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'raw_vector_df.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m     \"\"\"\n\u001b[1;32m   1435\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "GwlfR9YQZ45H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['LR_main_field'].unique())\n",
        "print(\"Number of unique publication years:\", df['pub_year'].nunique())\n",
        "print(\"Number of unique clusters:\", df['cluster_id1'].nunique())\n",
        "print(\"Number of unique researchers:\", df['researcher_id'].nunique())\n",
        "print(\"Number of unique institutions:\", df['grid_id'].nunique())"
      ],
      "metadata": {
        "id": "z8-o2hbgar9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create separate DataFrames for each category in 'LR_main_field'\n",
        "import os\n",
        "\n",
        "df_biomedical = df[df['LR_main_field'] == 'Biomedical and health sciences']\n",
        "df_physical = df[df['LR_main_field'] == 'Physical sciences and engineering']\n",
        "df_life_earth = df[df['LR_main_field'] == 'Life and earth sciences']\n",
        "df_social = df[df['LR_main_field'] == 'Social sciences and humanities']\n",
        "df_math_cs = df[df['LR_main_field'] == 'Mathematics and computer science']\n",
        "\n",
        "dir_path = \"/content/drive/My Drive/revisions_natcomms/data/\"\n",
        "\n",
        "df_biomedical.to_csv(dir_path + 'biomedical_and_health_sciences.csv', index=False)\n",
        "df_physical.to_csv(dir_path + 'physical_sciences_and_engineering.csv', index=False)\n",
        "df_life_earth.to_csv(dir_path + 'life_and_earth_sciences.csv', index=False)\n",
        "df_social.to_csv(dir_path + 'social_sciences_and_humanities.csv', index=False)\n",
        "df_math_cs.to_csv(dir_path + 'mathematics_and_computer_science.csv', index=False)"
      ],
      "metadata": {
        "id": "4hlx2wfeY2j6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the columns to keep and their new names\n",
        "columns_to_keep = {'researcher_id': 'aut', 'grid_id': 'ins', 'cluster_id1': 'sk', 'pubs': 'w'}\n",
        "\n",
        "def split_and_save(df, field_name, dir_path):\n",
        "    for category in ['native', 'newcomer', 'outgoer']:\n",
        "        df_filtered = df[df[category] == 1][list(columns_to_keep.keys())].rename(columns=columns_to_keep)\n",
        "        file_name = f'df_{field_name}_{category}.csv'\n",
        "        df_filtered.to_csv(dir_path + file_name, index=False)\n",
        "\n",
        "\n",
        "split_and_save(df_biomedical, 'biomedical', dir_path)\n",
        "split_and_save(df_physical, 'physical', dir_path)\n",
        "split_and_save(df_life_earth, 'life_earth', dir_path)\n",
        "split_and_save(df_social, 'social', dir_path)\n",
        "split_and_save(df_math_cs, 'math_cs', dir_path)"
      ],
      "metadata": {
        "id": "WbT2VgxTYZ8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Social sciences and humanities"
      ],
      "metadata": {
        "id": "6fZ4winahAW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title nat vs out\n",
        "# GET AUTHOR-INSTITUTION COSINE FOR NATIVES AND OUTGOERS\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "dir_path = \"/content/drive/My Drive/revisions_natcomms/data/\"\n",
        "df_social_native = load_df(dir_path + 'df_social_native.csv')\n",
        "df_social_outgoer = load_df(dir_path + 'df_social_outgoer.csv')\n",
        "\n",
        "unique_skills_native = df_social_native['sk'].unique()\n",
        "unique_skills_outgoer = df_social_outgoer['sk'].unique()\n",
        "combined_skills = set(unique_skills_native) | set(unique_skills_outgoer)\n",
        "global_unique_skills = list(combined_skills)\n",
        "\n",
        "grouped_native = df_social_native.groupby('ins')['sk'].apply(list).reset_index()\n",
        "grouped_outgoer = df_social_outgoer.groupby('ins')['sk'].apply(list).reset_index()\n",
        "\n",
        "native_skill_dict = {row['ins']: row['sk'] for _, row in grouped_native.iterrows()}\n",
        "outgoer_skill_dict = {row['ins']: row['sk'] for _, row in grouped_outgoer.iterrows()}\n",
        "\n",
        "def compute_cosine_similarity(skill_vector1, skill_vector2):\n",
        "    vector1 = np.zeros(len(global_unique_skills))\n",
        "    vector2 = np.zeros(len(global_unique_skills))\n",
        "\n",
        "    for skill_id in skill_vector1:\n",
        "        idx = global_unique_skills.index(skill_id)\n",
        "        vector1[idx] = 1\n",
        "\n",
        "    for skill_id in skill_vector2:\n",
        "        idx = global_unique_skills.index(skill_id)\n",
        "        vector2[idx] = 1\n",
        "\n",
        "    similarity = cosine_similarity([vector1], [vector2])[0][0]\n",
        "    return similarity\n",
        "\n",
        "cosine_similarities = {}\n",
        "for institution in native_skill_dict.keys():\n",
        "    native_skills = native_skill_dict[institution]\n",
        "    outgoer_skills = outgoer_skill_dict.get(institution, [])\n",
        "\n",
        "    similarity_score = compute_cosine_similarity(native_skills, outgoer_skills)\n",
        "    cosine_similarities[institution] = similarity_score"
      ],
      "metadata": {
        "id": "DZaoEzu8zS8u"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GQjSWXLvzTK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title not run\n",
        "#%%GET AUTHOR-INSTITUTION SKILLS MATRIX FOR NATIVES AND OUTGOERS\n",
        "\n",
        "out = create_df(data.loc[(data['outgoer'] == 1) & (data['LR_main_field']== 'Biomedical and health sciences')].rename(columns={'researcher_id': 'aut', 'grid_id': 'inst', 'cluster_id1':'sk'}))\n",
        "M_out_as, unique_sk_out, nodes_ins_out = create_matrix(out)\n",
        "\n",
        "nat = create_df(data.loc[(data['native'] == 1) & (data['LR_main_field'] == 'Biomedical and health sciences')].rename(columns={'researcher_id': 'aut', 'grid_id': 'inst', 'cluster_id1':'sk'}))\n",
        "M_res_as, unique_sk_res, nodes_ins_res = create_matrix(nat)\n",
        "\n",
        "\n",
        "#df = create_df('C:/Documents/sci_mobility/homophily_adaptation/datasets/shuffle/outgoers.csv')\n",
        "#M_out_as, unique_sk_out, nodes_ins_out = create_matrix(df)\n",
        "\n",
        "#%%GET MAPPINGS FROM NODES TO UNIQUE_INS\n",
        "unique_ins = np.unique(nodes_ins_out)\n",
        "\n",
        "ins2nodes_res = {}\n",
        "for i, x in enumerate(unique_ins):\n",
        "    print(i)\n",
        "    idx = np.where(nodes_ins_res == x)[0]\n",
        "    ins2nodes_res[x] = idx\n",
        "\n",
        "ins2nodes_out = {}\n",
        "for i, x in enumerate(unique_ins):\n",
        "    print(i)\n",
        "    idx = np.where(nodes_ins_out == x)[0]\n",
        "    ins2nodes_out[x] = idx\n",
        "\n",
        "#%%PUT THE TWO MATRICES IN THE SAME FORMAT\n",
        "import scipy\n",
        "\n",
        "unique_sk_res = [int(x) for x in unique_sk_res]\n",
        "unique_sk_out = [int(x) for x in unique_sk_out]\n",
        "\n",
        "M_out_as_clean = scipy.sparse.lil_matrix((M_out_as.shape[0], len(unique_sk_res)))\n",
        "M_res_as = M_res_as.tolil()\n",
        "\n",
        "for i, idx in enumerate(unique_sk_out):\n",
        "    print(i, idx)\n",
        "    M_out_as_clean[:, idx] = M_out_as[:, i]\n",
        "\n",
        "#%% GET ORIGINAL M_IS FOR RESIDENTS AND OUTGOERS\n",
        "M_out_as = M_out_as_clean.tocsr()\n",
        "M_res_as = M_res_as.tocsr()\n",
        "\n",
        "idx = list(range(len(nodes_ins_res)))\n",
        "M_res_is = np.zeros((len(unique_ins), M_res_as.shape[1]))\n",
        "for i, x in enumerate(unique_ins):\n",
        "    M_res_is[i, :] = M_res_as[ins2nodes_res[x], :].sum(axis=0)\n",
        "\n",
        "idx = list(range(len(nodes_ins_out)))\n",
        "M_out_is = np.zeros((len(unique_ins), M_out_as.shape[1]))\n",
        "for i, x in enumerate(unique_ins):\n",
        "    M_out_is[i, :] = M_out_as[ins2nodes_out[x], :].sum(axis=0)\n",
        "\n",
        "#%% COMPUTE COSINES OF UNSHUFFLED CASE\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "cs = [dot(M_out_is[i, :], M_res_is[i, :])/(norm((M_out_is[i, :])*norm(M_res_is[i, :]))) for i in range(len(unique_ins))]"
      ],
      "metadata": {
        "id": "q0jRDF50W4Ww"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}